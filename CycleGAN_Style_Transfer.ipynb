{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"},{"sourceId":7108618,"sourceType":"datasetVersion","datasetId":4098471}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# 1. Project Topic\n","\n","GitHub Repo: https://github.com/wr0b1n/MSDS-5511-Week5\n","\n","In this competition, we will use Generative Adversarial Networks (GANs) to recreate Claude Monet's artistic style in images. GANs consist of a generator and discriminator, where the generator produces Monet-style images, and the discriminator distinguishes real from generated ones. The goal is to generate 7,000 to 10,000 Monet-style images that will be evaluated on the MiFID (Memorization-informed Fr√©chet Inception Distance).\n","\n","Generative Deep Learning models are a subset of deep learning models designed to generate new data samples that resemble a given dataset. These models are used in various applications, such as image generation, text generation, and more. The key idea behind generative models is to learn the underlying distribution of the training data and then sample from this distribution to create new data samples.\n","\n","GANs in particular are a powerful class of generative models that consist of two neural networks, namely a generator and a discriminator that are trained adversarially. The goal of the generator is to create realistic data samples, while the discriminator tries to decide between real and fake samples. This adversarial training process results in the generator improving its ability to produce high-quality, realistic data."],"metadata":{"id":"ogtu63ldNEyv"}},{"cell_type":"markdown","source":["# 2. Data\n","\n","We are provided with monet paintings both in JPG and TFRecords format as our training data. In case of the JPG data we have about 300 image files. For generating new images we are provided with 7038 photos (again in JPG and TFRecords format). These photos will be manipulated by our model in order get the monet style and make them look like real monet paintings.\n","\n","Since we are working with a large amount of image data we will probably face extremely long computation time. For this reason we will need to activate the TPU accelerator in the Kaggle notebook.\n","\n","Both the monet paintings as well as the photos have a size of 256x256 pixels. The submission format is a zip-file containing 7,000-10,000 images sized 256x256 as well."],"metadata":{"id":"nca7k8DWNEy2"}},{"cell_type":"markdown","source":["# 3. Import Python Libraries\n","\n","Let's import the most important libraries for our project."],"metadata":{"id":"FKoIWILJNEy2"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","import math\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as init\n","from torch.utils.data import Dataset, random_split, DataLoader\n","\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","\n","from tqdm.notebook import tqdm\n","import itertools\n","import time\n","import shutil"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:46.978133Z","iopub.execute_input":"2023-12-05T16:39:46.978446Z","iopub.status.idle":"2023-12-05T16:39:51.114537Z","shell.execute_reply.started":"2023-12-05T16:39:46.978419Z","shell.execute_reply":"2023-12-05T16:39:51.113605Z"},"trusted":true,"id":"ev-bYqMONEy6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Function to calculate tensor size\n","def calculate_size(input_size, layer):\n","    with torch.no_grad():\n","        input_tensor = torch.randn(input_size)\n","        output_tensor = layer(input_tensor)\n","    return output_tensor.size()\n","\n","# Model architecture\n","in_ch = 3\n","out_ch = 3\n","\n","model = [\n","    nn.Conv2d(in_ch, 64, 3, 2, 1, bias=True),\n","    nn.ReLU(),\n","    nn.Conv2d(64, 128, 3, 1, 1, bias=True),\n","    nn.Dropout(0.3),\n","    nn.LeakyReLU(negative_slope=0.01, inplace=True),\n","    nn.BatchNorm2d(128),\n","    nn.Conv2d(128, 256, 5, 2, 1, bias=True),\n","    nn.LeakyReLU(negative_slope=0.03, inplace=True),\n","    nn.InstanceNorm2d(256),\n","    nn.ReflectionPad2d(1),\n","    nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1),\n","    nn.ReLU(),\n","    nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=0),\n","    nn.Dropout(0.2),\n","    nn.LeakyReLU(negative_slope=0.01, inplace=True),\n","    nn.BatchNorm2d(64),\n","    nn.Conv2d(64, out_ch, kernel_size=6, padding=1),\n","    nn.Tanh(),\n","]\n","\n","# Input size\n","input_size = (1, in_ch, 256, 256)\n","\n","# Calculate and print sizes for each layer\n","for layer in model:\n","    output_size = calculate_size(input_size, layer)\n","    print(f\"Layer: {layer.__class__.__name__}, Target Size: {output_size}\")\n","    input_size = output_size\n"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:51.116847Z","iopub.execute_input":"2023-12-05T16:39:51.117751Z","iopub.status.idle":"2023-12-05T16:39:51.879223Z","shell.execute_reply.started":"2023-12-05T16:39:51.117711Z","shell.execute_reply":"2023-12-05T16:39:51.878096Z"},"trusted":true,"id":"4DWuCd2NNEy8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. EDA and Data Preprocessing\n","\n","First, we have to determine the paths were our image data is located and have a look at how many images we are working with."],"metadata":{"id":"XcdCJIObNEy8"}},{"cell_type":"code","source":["# access folders\n","path_monet = \"../input/gan-getting-started/monet_jpg/\"\n","path_photo = \"../input/gan-getting-started/photo_jpg/\"\n","\n","print(\"Number of monet paintings: {}\".format(len(os.listdir(path_monet))))\n","print(\"Number of photos: {}\".format(len(os.listdir(path_photo))))"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:51.880334Z","iopub.execute_input":"2023-12-05T16:39:51.880630Z","iopub.status.idle":"2023-12-05T16:39:52.168225Z","shell.execute_reply.started":"2023-12-05T16:39:51.880604Z","shell.execute_reply":"2023-12-05T16:39:52.167305Z"},"trusted":true,"id":"Kk2cYHMkNEy9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's display some example images, both monet and real photos."],"metadata":{"id":"AqIpdyoiNEy-"}},{"cell_type":"markdown","source":["As we can see monet paintings have that distinctive style that many of us instantly relate to the famous painter called Monet. The photos on the other hand seem to be taken with a normal camera. Both types of images are colored ones, not just black and white.\n","\n","We can verify the stated image sizes from the kaggle description by inspecting the width and height of both the monet paintings as well as the real photos."],"metadata":{"id":"Z0Hq2_VJNEy-"}},{"cell_type":"markdown","source":["As we can see in the histograms all images are indeed of size 256x256 pixels. Next, we will have a look at how the different color channels are distributed in the images."],"metadata":{"id":"GjAsMKBJNEy_"}},{"cell_type":"markdown","source":["The above plots show the color channels for three monet paintings respectively photos. We could now ask the question if a certain color channel is more present in the monet paintings than in the photos."],"metadata":{"id":"riJngzUzNEy_"}},{"cell_type":"markdown","source":["The bar plot shows us that all color channels (Red, Green, Blue) have higher average values in Monet's paintings compared to normal photos. This makes sense since Monet is known for his impressionist style which often uses intense colors to capture the play of light and atmosphere. They tend to have a richer and more saturated color palette. This characteristic aligns with Monet's artistic style, where he emphasized the use of color to show emotions and capture the essence of a scene rather than adhering to strict color realism. Now that we are fimiliar with the given data we can begin with model building."],"metadata":{"id":"gMpswaBONEzA"}},{"cell_type":"markdown","source":["# 5. Model Building\n","\n","As we already know Generative Adversarial Network (GAN) is a type of machine learning model consisting of a generator and a discriminator that compete against each other to produce realistic data. In this project we will work with CycleGAN which is a specific GAN variant used for image-to-image translation tasks without paired data. It employs two generators and two discriminators to enable translating images from one domain to another while ensuring that the translations are consistent when reversed. This is commonly used for tasks like style transfer and image transformation.\n","\n","Since we are working with neural networks and image data we will make use of the Kaggle GPU to reduce running time of the notebook."],"metadata":{"id":"sSl0TrURNEzA"}},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device used for modeling: \", device)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.170444Z","iopub.execute_input":"2023-12-05T16:39:52.170739Z","iopub.status.idle":"2023-12-05T16:39:52.202962Z","shell.execute_reply.started":"2023-12-05T16:39:52.170714Z","shell.execute_reply":"2023-12-05T16:39:52.202103Z"},"trusted":true,"id":"lWUitrHwNEzB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.1 Dataset and Dataloader\n","\n","First we create a dataset for training our generative model. It takes the two directories where our monet paintings and photos can be found. The class loads and preprocesses the images, applying resizing and optionally normalization to create a consistent input for our neural network. With the __getitem__ method, we can randomly select a Monet-style image and the corresponding photo, processes them, and returns them as tensors. The __len__ method returns the length of the smaller of the two sets of images, ensuring that the dataset is limited by the number of available pairs."],"metadata":{"id":"S5D_aJlDNEzB"}},{"cell_type":"code","source":["class ImageDataset(Dataset):\n","    def __init__(self, path_monet, path_photo, size=(256, 256), normalize=True):\n","        super().__init__()\n","        self.monet_dir = path_monet\n","        self.photo_dir = path_photo\n","        self.monet_idx = dict()\n","        self.photo_idx = dict()\n","\n","        # normalize the images for consistency\n","        if normalize:\n","            self.transform = transforms.Compose([\n","                transforms.Resize(size),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","            ])\n","        else:\n","            self.transform = transforms.Compose([\n","                transforms.Resize(size),\n","                transforms.ToTensor()])\n","\n","        for i, fl in enumerate(os.listdir(self.monet_dir)):\n","            self.monet_idx[i] = fl\n","        for i, fl in enumerate(os.listdir(self.photo_dir)):\n","            self.photo_idx[i] = fl\n","\n","    # randomnly select a pair of monet painting and photo\n","    def __getitem__(self, idx):\n","        rand_idx = int(np.random.uniform(0, len(self.monet_idx.keys())))\n","        photo_path = os.path.join(self.photo_dir, self.photo_idx[rand_idx])\n","        monet_path = os.path.join(self.monet_dir, self.monet_idx[idx])\n","        photo_img = Image.open(photo_path)\n","        photo_img = self.transform(photo_img)\n","        monet_img = Image.open(monet_path)\n","        monet_img = self.transform(monet_img)\n","        return photo_img, monet_img\n","\n","    # ensure same amount of available photos\n","    def __len__(self):\n","        return min(len(self.monet_idx.keys()), len(self.photo_idx.keys()))"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.204458Z","iopub.execute_input":"2023-12-05T16:39:52.205032Z","iopub.status.idle":"2023-12-05T16:39:52.216205Z","shell.execute_reply.started":"2023-12-05T16:39:52.204997Z","shell.execute_reply":"2023-12-05T16:39:52.215308Z"},"trusted":true,"id":"XwZWC_qmNEzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_ds = ImageDataset(path_monet, path_photo)\n","img_dl = DataLoader(img_ds, batch_size=1, pin_memory=True)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.217420Z","iopub.execute_input":"2023-12-05T16:39:52.217695Z","iopub.status.idle":"2023-12-05T16:39:52.231914Z","shell.execute_reply.started":"2023-12-05T16:39:52.217671Z","shell.execute_reply":"2023-12-05T16:39:52.230918Z"},"trusted":true,"id":"9tjnm5DkNEzC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following is a helper method that performs the reverse operation of normalizing an image. Since our model only works with normalized versions of the images, this is needed for plotting the original version of the images after modeling them."],"metadata":{"id":"inAjmyywNEzC"}},{"cell_type":"code","source":["# reverse the normalization process\n","def reverse_normalize(img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n","    for image, mu, std in zip(img, mean, std):\n","        image.mul_(std).add_(std)\n","\n","    return img"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.232960Z","iopub.execute_input":"2023-12-05T16:39:52.233214Z","iopub.status.idle":"2023-12-05T16:39:52.238602Z","shell.execute_reply.started":"2023-12-05T16:39:52.233192Z","shell.execute_reply":"2023-12-05T16:39:52.237673Z"},"trusted":true,"id":"CKNxebBaNEzC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.2 Model building blocks\n","\n","The upsampling part is important for increasing the spatial resolution of an image. It is typically used in the generator network to gradually upsample the feature maps, which helps in generating higher-resolution output images from lower-resolution input images.\n","\n","The purpose of the following method is to define the logic for upsampling within a neural network. The choice of whether to use dropout or not is controlled by the use_dropout argument, providing more flexibility in designing the generator architecture."],"metadata":{"id":"oou1Eh31NEzD"}},{"cell_type":"code","source":["# method for upsampling feature maps\n","def Upsample(in_ch, out_ch, use_dropout=True, dropout_ratio=0.5):\n","    if use_dropout:\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(in_ch, out_ch, 3, stride=2, padding=1, output_padding=1),\n","            nn.InstanceNorm2d(out_ch),\n","            nn.Dropout(dropout_ratio),\n","            nn.GELU())\n","    else:\n","        return nn.Sequential(\n","            nn.ConvTranspose2d(in_ch, out_ch, 3, stride=2, padding=1, output_padding=1),\n","            nn.InstanceNorm2d(out_ch),\n","            nn.GELU())"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.239768Z","iopub.execute_input":"2023-12-05T16:39:52.240031Z","iopub.status.idle":"2023-12-05T16:39:52.252558Z","shell.execute_reply.started":"2023-12-05T16:39:52.240008Z","shell.execute_reply":"2023-12-05T16:39:52.251761Z"},"trusted":true,"id":"YHenJy_7NEzD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we need the convolutional layer which is responsible for extracting features from the input images. These layers apply convolution operations to process the image data and capture important patterns and details. They are used both in the generator and discriminator networks.\n","\n","The below method returns a PyTorch Sequential object that can be used to create the architecture of both the generator and discriminator in a CycleGAN model. The flexibility provided by the function's arguments allows for different architectural choices."],"metadata":{"id":"vmbLHOb3NEzD"}},{"cell_type":"code","source":["# define a convolutional layer with options for padding and more\n","def Convlayer(in_ch, out_ch, kernel_size=3, stride=2, use_leaky=True, use_inst_norm=True, use_pad=True):\n","    if use_pad:\n","        conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, 1, bias=True)\n","    else:\n","        conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, 0, bias=True)\n","\n","    if use_leaky:\n","        actv = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","    else:\n","        actv = nn.GELU()\n","\n","    if use_inst_norm:\n","        norm = nn.InstanceNorm2d(out_ch)\n","    else:\n","        norm = nn.BatchNorm2d(out_ch)\n","\n","    return nn.Sequential(conv, norm, actv)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.253601Z","iopub.execute_input":"2023-12-05T16:39:52.253927Z","iopub.status.idle":"2023-12-05T16:39:52.262647Z","shell.execute_reply.started":"2023-12-05T16:39:52.253904Z","shell.execute_reply":"2023-12-05T16:39:52.261914Z"},"trusted":true,"id":"xiKvohsaNEzD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Residual Block is part of the generator network. Its purpose is to make the network learn residual functions, which help preserve important details and prevent vanishing gradients during training. This is particularly useful for maintaining image quality and consistency during the translation process."],"metadata":{"id":"BBWiHjkUNEzE"}},{"cell_type":"code","source":["# defines the residual block for the generator part\n","class Resblock(nn.Module):\n","    def __init__(self, in_features, use_dropout=True, dropout_ratio=0.5):\n","        super().__init__()\n","        layers = list()\n","        layers.append(nn.ReflectionPad2d(1))\n","        layers.append(Convlayer(in_features, in_features, 3, 1, False, use_pad=False))\n","        layers.append(nn.Dropout(dropout_ratio))\n","        layers.append(nn.ReflectionPad2d(1))\n","        layers.append(nn.Conv2d(in_features, in_features, 3, 1, padding=0, bias=True))\n","        layers.append(nn.InstanceNorm2d(in_features))\n","        self.res = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return x + self.res(x)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.265776Z","iopub.execute_input":"2023-12-05T16:39:52.266014Z","iopub.status.idle":"2023-12-05T16:39:52.273256Z","shell.execute_reply.started":"2023-12-05T16:39:52.265993Z","shell.execute_reply":"2023-12-05T16:39:52.272330Z"},"trusted":true,"id":"53GHPW53NEzE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The generator is the component responsible for translating images from one domain to another. It takes an input image (e.g., a photo) and transforms it into an image that mimics the style or characteristics of another domain (e.g., a Monet painting). The purpose of the generator is to learn a mapping between these domains and generate high-quality images in the target domain.\n","\n","The class below represents this generator and defines the architecture and operations needed to perform the image translation task. It makes use of the Upsampling, Convolutional Layers, and ResBlock which we defined above."],"metadata":{"id":"8YQ7A66kNEzE"}},{"cell_type":"code","source":["# defines the generator class built upon convolutional layers\n","class Generator(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        \"\"\"\n","        model = list()\n","        model.append(nn.ReflectionPad2d(3))\n","        model.append(Convlayer(in_ch, 64, 7, 1, False, True, False))\n","        model.append(Convlayer(64, 128, 3, 2, False))\n","        model.append(Convlayer(128, 256, 3, 2, False))\n","        for _ in range(num_res_blocks):\n","            model.append(Resblock(256))\n","        model.append(Upsample(256, 128))\n","        model.append(Upsample(128, 64))\n","        model.append(nn.ReflectionPad2d(3))\n","        model.append(nn.Conv2d(64, out_ch, kernel_size=7, padding=0))\n","        model.append(nn.Tanh())\n","        \"\"\"\n","        model = list()\n","        model.append(nn.Conv2d(in_ch, 64, 3, 2, 1, bias=True))\n","        model.append(nn.ReLU())\n","        model.append(nn.Conv2d(64, 128, 3, 1, 1, bias=True))\n","        model.append(nn.Dropout(0.3))\n","        model.append(nn.LeakyReLU(negative_slope=0.01,inplace=True))\n","        model.append(nn.BatchNorm2d(128))\n","        model.append(nn.Conv2d(128,256,5,2,1,bias=True))\n","        model.append(nn.LeakyReLU(negative_slope=0.03,inplace=True))\n","        model.append(nn.InstanceNorm2d(256))\n","        model.append(nn.ReflectionPad2d(1))\n","        model.append(nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1))\n","        model.append(nn.ReLU())\n","        model.append(nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=0))\n","        model.append(nn.Dropout(0.2))\n","        model.append(nn.LeakyReLU(negative_slope=0.01,inplace=True))\n","        model.append(nn.BatchNorm2d(64))\n","        model.append(nn.Conv2d(64, out_ch, kernel_size=6, padding=1))\n","        model.append(nn.Tanh())\n","        self.gen = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        return self.gen(x)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.274253Z","iopub.execute_input":"2023-12-05T16:39:52.274543Z","iopub.status.idle":"2023-12-05T16:39:52.287919Z","shell.execute_reply.started":"2023-12-05T16:39:52.274520Z","shell.execute_reply":"2023-12-05T16:39:52.286974Z"},"trusted":true,"id":"qKjt92hqNEzE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The discriminator is the component responsible for distinguishing between real and generated images. Its purpose is to provide feedback to the generator by assessing the realism of the generated images and helping the generator improve over time. The discriminator is trained to classify input images as either real (belonging to the target domain) or fake (generated by the generator)."],"metadata":{"id":"zRMFepDwNEzF"}},{"cell_type":"code","source":["# defines the discriminator component\n","class Discriminator(nn.Module):\n","    def __init__(self, in_ch, num_layers=4):\n","\n","        super().__init__()\n","        model = list()\n","        \"\"\"\n","        model.append(nn.Conv2d(in_ch, 64, 4, stride=2, padding=1))\n","        model.append(nn.LeakyReLU(0.2, inplace=True))\n","        for i in range(1, num_layers):\n","            in_chs = 64 * 2**(i-1)\n","            out_chs = in_chs * 2\n","            if i == num_layers -1:\n","                model.append(Convlayer(in_chs, out_chs, 4, 1))\n","            else:\n","                model.append(Convlayer(in_chs, out_chs, 4, 2))\n","        model.append(nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1))\n","        \"\"\"\n","        model.append(nn.Conv2d(in_ch, 64,3,stride=1,padding=1))\n","        model.append(nn.Dropout(0.1))\n","        model.append(nn.ReLU())\n","        model.append(nn.BatchNorm2d(64))\n","        model.append(nn.Conv2d(64,128,4,stride=2,padding=1))\n","        model.append(nn.LeakyReLU(negative_slope=0.02,inplace=True))\n","        model.append(nn.Conv2d(128,1,4,stride=1,padding=1))\n","\n","\n","        self.disc = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        return self.disc(x)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.289035Z","iopub.execute_input":"2023-12-05T16:39:52.289318Z","iopub.status.idle":"2023-12-05T16:39:52.301665Z","shell.execute_reply.started":"2023-12-05T16:39:52.289289Z","shell.execute_reply":"2023-12-05T16:39:52.300756Z"},"trusted":true,"id":"c-PxeiWqNEzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As with deep learning in general we also need a weight initialization in CycleGAN to set the initial values of the model's weights in a way that helps improve the training process and convergence speed. This is really important since a proper weight initialization can prevent issues like vanishing gradients and allow the model to learn more effectively."],"metadata":{"id":"qqQCuduJNEzF"}},{"cell_type":"code","source":["# initalize with normally distributed weights around mean 0 and standard deviation of 0.02\n","def init_weights(net, init_type='normal', std=0.02):\n","    def init_func(m):\n","        classname = m.__class__.__name__\n","        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n","            init.normal_(m.weight.data, 0.0, std)\n","            if hasattr(m, 'bias') and m.bias is not None:\n","                init.constant_(m.bias.data, 0.0)\n","        elif classname.find('BatchNorm2d') != -1:\n","            init.normal_(m.weight.data, 1.0, std)\n","            init.constant_(m.bias.data, 0.0)\n","    net.apply(init_func)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.302727Z","iopub.execute_input":"2023-12-05T16:39:52.303000Z","iopub.status.idle":"2023-12-05T16:39:52.314988Z","shell.execute_reply.started":"2023-12-05T16:39:52.302977Z","shell.execute_reply":"2023-12-05T16:39:52.314123Z"},"trusted":true,"id":"2cOMNOHUNEzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.3 Cycle GAN\n","\n","First, we will need some additional helper methods that are explained and defined below.\n","\n","The purpose of the first function is to provide a convenient way to control whether the parameters of a list of models should be trainable (require gradients) or frozen (not require gradients) during training. This can be useful in scenarios where you want to fine-tune certain parts of a pre-trained model while keeping others fixed, or when you want to selectively train or freeze specific model components during different training phases."],"metadata":{"id":"aO5joBZXNEzG"}},{"cell_type":"code","source":["# decide whether certain model parameters should be trainable or not\n","def update_req_grad(models, requires_grad=True):\n","    for model in models:\n","        for param in model.parameters():\n","            param.requires_grad = requires_grad"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.316073Z","iopub.execute_input":"2023-12-05T16:39:52.316412Z","iopub.status.idle":"2023-12-05T16:39:52.323614Z","shell.execute_reply.started":"2023-12-05T16:39:52.316381Z","shell.execute_reply":"2023-12-05T16:39:52.322797Z"},"trusted":true,"id":"vEq1jpb7NEzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The purpose of the \"sample fake\" mechanism in CycleGAN is to help stabilize training by sampling a subset of generated fake images and using them as inputs to the discriminator. This approach is used to reduce the variability in the inputs to the discriminator, which can help avoid large oscillations in the discriminator's outputs from one training iteration to the next. By doing so, it can make the training process more stable and lead to better convergence.\n","\n","For this reason we provide a method seen below to control the number of fake images used for each training iteration and introduce a level of randomness in the selection of fake images to stabilize training in CycleGAN."],"metadata":{"id":"SDK25RJaNEzG"}},{"cell_type":"code","source":["# class to save 50 generated fake images and sample through them to feed discriminator\n","class sample_fake(object):\n","    def __init__(self, max_imgs=50):\n","        self.max_imgs = max_imgs\n","        self.cur_img = 0\n","        self.imgs = list()\n","\n","    def __call__(self, imgs):\n","        ret = list()\n","        for img in imgs:\n","            if self.cur_img < self.max_imgs:\n","                self.imgs.append(img)\n","                ret.append(img)\n","                self.cur_img += 1\n","            else:\n","                if np.random.ranf() > 0.5:\n","                    idx = np.random.randint(0, self.max_imgs)\n","                    ret.append(self.imgs[idx])\n","                    self.imgs[idx] = img\n","                else:\n","                    ret.append(img)\n","        return ret"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.324592Z","iopub.execute_input":"2023-12-05T16:39:52.324846Z","iopub.status.idle":"2023-12-05T16:39:52.336290Z","shell.execute_reply.started":"2023-12-05T16:39:52.324824Z","shell.execute_reply":"2023-12-05T16:39:52.335578Z"},"trusted":true,"id":"FGUNHZYBNEzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The purpose of learning rate scheduling is to dynamically adjust the learning rate during training to optimize the convergence of the neural network. Learning rate scheduling typically involves reducing the learning rate over time to help the model converge more smoothly and reach a better minimum of the loss function.\n","\n","By using this linear decay strategy, the learning rate gradually decreases as training progresses beyond **self.decay_epochs**. It's a simple yet effective way to improve training stability."],"metadata":{"id":"hMSPL_EcNEzH"}},{"cell_type":"code","source":["# class to schedule the learning rate during training\n","class lr_sched():\n","    def __init__(self, decay_epochs=100, total_epochs=200):\n","        self.decay_epochs = decay_epochs\n","        self.total_epochs = total_epochs\n","\n","    def step(self, epoch_num):\n","        if epoch_num <= self.decay_epochs:\n","            return 1.0\n","        else:\n","            fract = (epoch_num - self.decay_epochs)  / (self.total_epochs - self.decay_epochs)\n","            return 1.0 - fract\n","            #return 0.9 ** (epoch_num - self.decay_epochs)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.337350Z","iopub.execute_input":"2023-12-05T16:39:52.337650Z","iopub.status.idle":"2023-12-05T16:39:52.345837Z","shell.execute_reply.started":"2023-12-05T16:39:52.337627Z","shell.execute_reply":"2023-12-05T16:39:52.344950Z"},"trusted":true,"id":"ZNOpZX9ENEzH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we define a class that is able to collect and track important statistics, like loss values and iteration numbers, over the course of training. These statistics can be used for various purposes, such as monitoring the training progress, visualizing loss curves, or making decisions about model training."],"metadata":{"id":"Z18TVVdINEzH"}},{"cell_type":"code","source":["# class for saving some training metrics\n","class AvgStats(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.losses =[]\n","        self.its = []\n","\n","    def append(self, loss, it):\n","        self.losses.append(loss)\n","        self.its.append(it)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.346864Z","iopub.execute_input":"2023-12-05T16:39:52.347117Z","iopub.status.idle":"2023-12-05T16:39:52.355181Z","shell.execute_reply.started":"2023-12-05T16:39:52.347096Z","shell.execute_reply":"2023-12-05T16:39:52.354488Z"},"trusted":true,"id":"fQmyOKqSNEzH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following class defines the Cycle GAN itself together with the required training loop. It trains the model to translate images between the two domains (photos and Monet paintings). It includes the training logic for both generators and discriminators, learning rate scheduling, and tracking of training statistics. The model's losses are backpropagated, and the parameters are updated accordingly during training."],"metadata":{"id":"dAy_dZeSNEzO"}},{"cell_type":"code","source":["class StyleLoss(nn.Module):\n","    def __init__(self):\n","        super(StyleLoss, self).__init__()\n","\n","    def gram_matrix(self, x):\n","        b, c, h, w = x.size()\n","        features = x.view(b, c, h * w)\n","        gram = torch.bmm(features, features.transpose(1, 2)) / (c * h * w)\n","        return gram\n","\n","    def forward(self, x, target):\n","        style_loss = F.mse_loss(self.gram_matrix(x), self.gram_matrix(target))\n","        return style_loss\n"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.356334Z","iopub.execute_input":"2023-12-05T16:39:52.356921Z","iopub.status.idle":"2023-12-05T16:39:52.365205Z","shell.execute_reply.started":"2023-12-05T16:39:52.356897Z","shell.execute_reply":"2023-12-05T16:39:52.364316Z"},"trusted":true,"id":"BbGGi9ERNEzO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defines the Cycle GAN and its training loop\n","style_losses = {}\n","style_losses[\"style_loss_monet\"] = []\n","style_losses[\"style_loss_photo\"] = []\n","class CycleGAN(object):\n","    def __init__(self, in_ch, out_ch, epochs, device, start_lr=2e-4, lmbda=10, idt_coef=0.5, decay_epoch=0):\n","        self.epochs = epochs\n","        self.decay_epoch = decay_epoch if decay_epoch > 0 else int(self.epochs/2)\n","        self.lmbda = lmbda\n","        self.idt_coef = idt_coef\n","        self.device = device\n","        self.gen_mtp = Generator(in_ch, out_ch)\n","        self.gen_ptm = Generator(in_ch, out_ch)\n","        self.desc_m = Discriminator(in_ch)\n","        self.desc_p = Discriminator(in_ch)\n","        self.init_models()\n","        self.mse_loss = nn.MSELoss()\n","        self.l1_loss = nn.L1Loss()\n","        self.style_loss = StyleLoss()\n","#         self.adam_gen = torch.optim.Adam(itertools.chain(self.gen_mtp.parameters(), self.gen_ptm.parameters()),\n","#                                         lr = start_lr, betas=(0.5, 0.999))\n","#         self.adam_desc = torch.optim.Adam(itertools.chain(self.desc_m.parameters(), self.desc_p.parameters()),\n","#                                          lr=start_lr, betas=(0.5, 0.999))\n","        self.rms_gen = torch.optim.RMSprop(itertools.chain(self.gen_mtp.parameters(), self.gen_ptm.parameters()),\n","                                          lr=start_lr, alpha = 0.9)\n","        self.rms_desc = torch.optim.RMSprop(itertools.chain(self.desc_m.parameters(), self.desc_p.parameters()),\n","                                          lr=start_lr, alpha = 0.9)\n","#         self.sgd_gen = torch.optim.SGD(itertools.chain(self.gen_mtp.parameters(), self.gen_ptm.parameters()),\n","#                                 lr=start_lr, momentum=0.9)\n","#         self.sgd_desc = torch.optim.SGD(itertools.chain(self.desc_m.parameters(), self.desc_p.parameters()),\n","#                                 lr=start_lr, momentum=0.9)\n","        self.gen_optim = self.rms_gen\n","        self.desc_optim = self.rms_desc\n","        self.sample_monet = sample_fake()\n","        self.sample_photo = sample_fake()\n","        gen_lr = lr_sched(self.decay_epoch, self.epochs)\n","        desc_lr = lr_sched(self.decay_epoch, self.epochs)\n","        self.gen_lr_sched = torch.optim.lr_scheduler.LambdaLR(self.gen_optim, gen_lr.step)\n","        self.desc_lr_sched = torch.optim.lr_scheduler.LambdaLR(self.desc_optim, desc_lr.step)\n","        self.gen_stats = AvgStats()\n","        self.desc_stats = AvgStats()\n","\n","    def init_models(self):\n","        init_weights(self.gen_mtp)\n","        init_weights(self.gen_ptm)\n","        init_weights(self.desc_m)\n","        init_weights(self.desc_p)\n","        self.gen_mtp = self.gen_mtp.to(self.device)\n","        self.gen_ptm = self.gen_ptm.to(self.device)\n","        self.desc_m = self.desc_m.to(self.device)\n","        self.desc_p = self.desc_p.to(self.device)\n","\n","    def train(self, photo_dl):\n","        for epoch in range(self.epochs):\n","            start_time = time.time()\n","            avg_gen_loss = 0.0\n","            avg_desc_loss = 0.0\n","            t = tqdm(photo_dl, leave=False, total=photo_dl.__len__())\n","            for i, (photo_real, monet_real) in enumerate(t):\n","                photo_img, monet_img = photo_real.to(device), monet_real.to(device)\n","                update_req_grad([self.desc_m, self.desc_p], False)\n","                self.gen_optim.zero_grad()\n","\n","                # forward pass through generator\n","                fake_photo = self.gen_mtp(monet_img)\n","                fake_monet = self.gen_ptm(photo_img)\n","\n","                cycl_monet = self.gen_ptm(fake_photo)\n","                cycl_photo = self.gen_mtp(fake_monet)\n","\n","                id_monet = self.gen_ptm(monet_img)\n","                id_photo = self.gen_mtp(photo_img)\n","\n","                style_photo = self.gen_mtp(monet_img)\n","                style_monet = self.gen_ptm(photo_img)\n","\n","                # generator losses\n","                idt_loss_monet = self.l1_loss(id_monet, monet_img) * self.lmbda * self.idt_coef\n","                idt_loss_photo = self.l1_loss(id_photo, photo_img) * self.lmbda * self.idt_coef\n","\n","                cycle_loss_monet = self.l1_loss(cycl_monet, monet_img) * self.lmbda\n","                cycle_loss_photo = self.l1_loss(cycl_photo, photo_img) * self.lmbda\n","\n","                style_loss_monet = self.style_loss(style_monet,monet_img)\n","                style_loss_photo = self.style_loss(style_photo,photo_img)\n","\n","                style_losses[\"style_loss_monet\"].append(style_loss_monet)\n","                style_losses[\"style_loss_photo\"].append(style_loss_photo)\n","\n","                monet_desc = self.desc_m(fake_monet)\n","                photo_desc = self.desc_p(fake_photo)\n","\n","                real = torch.ones(monet_desc.size()).to(self.device)\n","\n","                adv_loss_monet = self.mse_loss(monet_desc, real)\n","                adv_loss_photo = self.mse_loss(photo_desc, real)\n","\n","                # total generator loss\n","                total_gen_loss = cycle_loss_monet + adv_loss_monet\\\n","                              + cycle_loss_photo + adv_loss_photo\\\n","                              + idt_loss_monet + idt_loss_photo\\\n","                                + style_loss_monet + style_loss_photo\n","\n","                avg_gen_loss += total_gen_loss.item()\n","\n","                # backward pass\n","                total_gen_loss.backward()\n","                self.gen_optim.step()\n","\n","                # forward pass through Descriminator\n","                update_req_grad([self.desc_m, self.desc_p], True)\n","                self.desc_optim.zero_grad()\n","\n","                fake_monet = self.sample_monet([fake_monet.cpu().data.numpy()])[0]\n","                fake_photo = self.sample_photo([fake_photo.cpu().data.numpy()])[0]\n","                fake_monet = torch.tensor(fake_monet).to(self.device)\n","                fake_photo = torch.tensor(fake_photo).to(self.device)\n","\n","                monet_desc_real = self.desc_m(monet_img)\n","                monet_desc_fake = self.desc_m(fake_monet)\n","                photo_desc_real = self.desc_p(photo_img)\n","                photo_desc_fake = self.desc_p(fake_photo)\n","\n","                real = torch.ones(monet_desc_real.size()).to(self.device)\n","                fake = torch.zeros(monet_desc_fake.size()).to(self.device)\n","\n","                # descriminator losses\n","                monet_desc_real_loss = self.mse_loss(monet_desc_real, real)\n","                monet_desc_fake_loss = self.mse_loss(monet_desc_fake, fake)\n","                photo_desc_real_loss = self.mse_loss(photo_desc_real, real)\n","                photo_desc_fake_loss = self.mse_loss(photo_desc_fake, fake)\n","\n","                monet_desc_loss = (monet_desc_real_loss + monet_desc_fake_loss) / 2\n","                photo_desc_loss = (photo_desc_real_loss + photo_desc_fake_loss) / 2\n","                total_desc_loss = monet_desc_loss + photo_desc_loss\n","                avg_desc_loss += total_desc_loss.item()\n","\n","                # backward\n","                monet_desc_loss.backward()\n","                photo_desc_loss.backward()\n","                self.desc_optim.step()\n","\n","                t.set_postfix(gen_loss=total_gen_loss.item(), desc_loss=total_desc_loss.item())\n","\n","            avg_gen_loss /= photo_dl.__len__()\n","            avg_desc_loss /= photo_dl.__len__()\n","            time_req = time.time() - start_time\n","\n","            self.gen_stats.append(avg_gen_loss, time_req)\n","            self.desc_stats.append(avg_desc_loss, time_req)\n","\n","            print(\"Epoch %d  -  Generator Loss:%f  -  Discriminator Loss:%f\" % (epoch+1, avg_gen_loss, avg_desc_loss))\n","\n","            self.gen_lr_sched.step()\n","            self.desc_lr_sched.step()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:39:52.366582Z","iopub.execute_input":"2023-12-05T16:39:52.366888Z","iopub.status.idle":"2023-12-05T16:39:52.396682Z","shell.execute_reply.started":"2023-12-05T16:39:52.366863Z","shell.execute_reply":"2023-12-05T16:39:52.395726Z"},"trusted":true,"id":"Ci6h1Q1gNEzP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.4 Training process\n","\n","Now that we have defined all necesarry building blocks we can finally train our model! We create a new object of our Cycle GAN class and start the training process with 50 epochs."],"metadata":{"id":"u4_YyV_HNEzP"}},{"cell_type":"code","source":["gan = CycleGAN(3, 3, 1, device)\n","gan.train(img_dl)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:40:13.957034Z","iopub.execute_input":"2023-12-05T16:40:13.957417Z","iopub.status.idle":"2023-12-05T16:41:13.922214Z","shell.execute_reply.started":"2023-12-05T16:40:13.957385Z","shell.execute_reply":"2023-12-05T16:41:13.921222Z"},"trusted":true,"id":"ckz0if6CNEzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["style_losses"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:42:12.821313Z","iopub.execute_input":"2023-12-05T16:42:12.822090Z","iopub.status.idle":"2023-12-05T16:42:13.212566Z","shell.execute_reply.started":"2023-12-05T16:42:12.822051Z","shell.execute_reply":"2023-12-05T16:42:13.211669Z"},"trusted":true,"id":"OCA3QoI8NEzQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's visualize the results."],"metadata":{"id":"SKpt2P1qNEzQ"}},{"cell_type":"code","source":["#plot losses\n","def plot_gan_loss():\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Losses\")\n","    plt.plot(gan.gen_stats.losses, 'r', label='Generator Loss')\n","    plt.plot(gan.desc_stats.losses, 'b', label='Descriminator Loss')\n","    plt.legend()\n","    plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:40:10.088316Z","iopub.status.idle":"2023-12-05T16:40:10.088650Z","shell.execute_reply.started":"2023-12-05T16:40:10.088484Z","shell.execute_reply":"2023-12-05T16:40:10.088499Z"},"trusted":true,"id":"TvcAdrghNEzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_gan_loss()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T16:40:10.090689Z","iopub.status.idle":"2023-12-05T16:40:10.091822Z","shell.execute_reply.started":"2023-12-05T16:40:10.091561Z","shell.execute_reply":"2023-12-05T16:40:10.091595Z"},"trusted":true,"id":"jztJ2WZZNEzR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from scipy import linalg\n","from torchvision.models import inception_v3\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","# Define a function to compute the FID between two sets of images\n","def calculate_frechet_distance(mu1, sigma1, mu2, sigma2):\n","    eps = 1e-6\n","    mu1, sigma1 = Variable(mu1), Variable(sigma1)\n","    mu2, sigma2 = Variable(mu2), Variable(sigma2)\n","\n","    diff = mu1 - mu2\n","\n","    covmean, _ = linalg.sqrtm(sigma1 @ sigma2, disp=False)\n","    if not np.isfinite(covmean).all():\n","        offset = np.eye(sigma1.shape[0]) * eps\n","        covmean = linalg.sqrtm((sigma1 + offset) @ (sigma2 + offset))\n","\n","    # Numerical error might give slight imaginary part. Remove it.\n","    if np.iscomplexobj(covmean):\n","        covmean = covmean.real\n","\n","    tr_covmean = np.trace(covmean)\n","\n","    return (diff @ diff + torch.trace(sigma1) + torch.trace(sigma2) - 2 * tr_covmean).item()\n","\n","# Define a function to calculate mFID for a given generator and real dataset\n","def calculate_mfid(generator, real_dataset, num_samples=1, batch_size=1):\n","    generator.eval()\n","\n","    # Generate fake samples\n","    fake_samples = []\n","    with torch.no_grad():\n","        for i, (content_img, _) in enumerate(real_dataset):  # Assuming your DataLoader returns (content_img, style_img)\n","            content_img = content_img.to(device)\n","            generated_images = generator(content_img)\n","            fake_samples.append(generated_images.cpu().numpy())\n","\n","            # Break the loop if enough samples are generated\n","            if i * batch_size >= num_samples:\n","                break\n","\n","    fake_samples = np.concatenate(fake_samples, axis=0)\n","\n","    # Load real dataset (assumes real_dataset is a PyTorch DataLoader)\n","    real_samples = []\n","    for batch in real_dataset:\n","        real_samples.append(batch[0].numpy())\n","\n","    real_samples = np.concatenate(real_samples, axis=0)\n","\n","\n","    # Compute FID statistics for both real and fake samples\n","#     real_mu, real_sigma = np.mean(real_samples, axis=0), np.cov(real_samples, rowvar=False)\n","#     fake_mu, fake_sigma = np.mean(fake_samples, axis=0), np.cov(fake_samples, rowvar=False)\n","\n","    real_samples_2d = real_samples.reshape(real_samples.shape[0], -1)\n","    fake_samples_2d = fake_samples.reshape(fake_samples.shape[0], -1)\n","\n","    # Compute FID statistics for both real and fake samples\n","    real_mu, real_sigma = np.mean(real_samples_2d, axis=0), np.cov(real_samples_2d, rowvar=False)\n","    fake_mu, fake_sigma = np.mean(fake_samples_2d, axis=0), np.cov(fake_samples_2d, rowvar=False)\n","\n","    # Compute FID\n","    fid = calculate_frechet_distance(real_mu, real_sigma, fake_mu, fake_sigma)\n","\n","    # Optional: Add memorization penalty (requires additional implementation)\n","    # memorization_penalty = calculate_memorization_penalty(generator, real_dataset)\n","    # mfid = fid + lambda_memorization * memorization_penalty\n","\n","    return fid\n","\n","# Example usage:\n","real_dataset = img_dl  # Provide your real dataset DataLoader here\n","generator = gan.gen_ptm # Replace with your actual generator model\n","fid = calculate_mfid(generator, real_dataset)\n","print(f\"mFID: {fid}\")\n"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:22:07.882537Z","iopub.execute_input":"2023-12-05T03:22:07.882826Z"},"trusted":true,"id":"Lfcr9pVENEzR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that both loss functions gradually decrease over time which is what we would expect from a working model. In order to see the actual results we can plot same random photos and the corresponding Monet-esque Photos created by our model."],"metadata":{"id":"r01muOLRNEzR"}},{"cell_type":"markdown","source":["### Visualizing the neuron in the generator"],"metadata":{"id":"x1OfgVU7NEzR"}},{"cell_type":"code","source":["# # defines the generator class built upon convolutional layers\n","# class Visualize_Generator(nn.Module):\n","#     def __init__(self, in_ch, out_ch):\n","#         super().__init__()\n","#         \"\"\"\n","#         model = list()\n","#         model.append(nn.ReflectionPad2d(3))\n","#         model.append(Convlayer(in_ch, 64, 7, 1, False, True, False))\n","#         model.append(Convlayer(64, 128, 3, 2, False))\n","#         model.append(Convlayer(128, 256, 3, 2, False))\n","#         for _ in range(num_res_blocks):\n","#             model.append(Resblock(256))\n","#         model.append(Upsample(256, 128))\n","#         model.append(Upsample(128, 64))\n","#         model.append(nn.ReflectionPad2d(3))\n","#         model.append(nn.Conv2d(64, out_ch, kernel_size=7, padding=0))\n","#         model.append(nn.Tanh())\n","#         \"\"\"\n","#         model = list()\n","#         model.append(nn.Conv2d(in_ch, 64, 3, 2, 1, bias=True))\n","#         model.append(nn.ReLU())\n","\n","#         model.append(nn.Conv2d(64, 128, 3, 1, 1, bias=True))\n","\n","#         model.append(nn.Dropout(0.3))\n","#         model.append(nn.LeakyReLU(negative_slope=0.01,inplace=True))\n","#         model.append(nn.BatchNorm2d(128))\n","#         model.append(nn.Conv2d(128,256,5,2,1,bias=True))\n","\n","# #         model.append(nn.LeakyReLU(negative_slope=0.03,inplace=True))\n","# #         model.append(nn.InstanceNorm2d(256))\n","# #         model.append(nn.ReflectionPad2d(1))\n","# #         model.append(nn.ConvTranspose2d(256, 128, 3, stride=2, padding=1, output_padding=1))\n","# #         model.append(nn.ReLU())\n","\n","# #         model.append(nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=0))\n","# #         model.append(nn.Dropout(0.2))\n","# #         model.append(nn.LeakyReLU(negative_slope=0.01,inplace=True))\n","# #         model.append(nn.BatchNorm2d(64))\n","# #         model.append(nn.Conv2d(64, out_ch, kernel_size=6, padding=1))\n","# #         model.append(nn.Tanh())\n","#         self.gen = nn.Sequential(*model)\n","\n","#     def forward(self, x):\n","#         return self.gen(x)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:05:40.838297Z","iopub.status.idle":"2023-12-02T22:05:40.838700Z","shell.execute_reply.started":"2023-12-02T22:05:40.838519Z","shell.execute_reply":"2023-12-02T22:05:40.838538Z"},"trusted":true,"id":"VuRRflHcNEzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generator = Visualize_Generator(in_ch=3, out_ch=3)\n","\n","# # Set the model to training mode (important for dropout layers)\n","# generator.train()\n","\n","# # Create a random input image\n","# #input_image = torch.randn((1, 3, 256, 256), requires_grad=True)\n","# image_path = \"../input/gan-getting-started/photo_jpg/00068bc07f.jpg\"\n","# image = Image.open(image_path).convert(\"RGB\")\n","\n","# # Define image transformations to match the generator's input requirements\n","# transform = transforms.Compose([\n","#     transforms.Resize((256, 256)),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","# ])\n","\n","# # Apply transformations to the image\n","# input_image = transform(image).unsqueeze(0).requires_grad_()\n","\n","# # Forward pass\n","# output_image = generator(input_image)\n","\n","# # Choose a specific layer and neuron for which you want to retain the gradient\n","# target_layer_index = 8  # Choose the index of the desired layer\n","# neuron_index = 0 # Choose the index of the neuron you want to retain the gradient for\n","\n","# # Backward pass\n","# output_image[:, neuron_index, :, :].sum().backward(retain_graph=True)\n","\n","# # Set gradients to zero for all neurons in the chosen layer except the specified neuron\n","# for i, layer in enumerate(generator.gen):\n","#     if i == target_layer_index:\n","#         for param in layer.parameters():\n","#             if param.grad is not None:\n","#                 param.grad[:, :, :, :] = 0.0\n","#                 param.grad[:, neuron_index, :, :].sum().backward()\n","\n","# # Get the backpropagated image\n","# backprop_image = input_image.grad.data.numpy()[0]\n","\n","# # Normalize the image values to be in the range [0, 1]\n","# backprop_image = (backprop_image - backprop_image.min()) / (backprop_image.max() - backprop_image.min())\n","\n","# # Display the original and backpropagated images\n","# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","# axes[0].imshow(np.transpose(input_image.detach().numpy()[0], (1, 2, 0)))\n","# axes[0].set_title('Original Image')\n","# axes[0].axis('off')\n","# axes[1].imshow(np.transpose(backprop_image, (1, 2, 0)))\n","# axes[1].set_title('Backpropagated Image')\n","# axes[1].axis('off')\n","# plt.show()\n"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:05:40.840296Z","iopub.status.idle":"2023-12-02T22:05:40.840640Z","shell.execute_reply.started":"2023-12-02T22:05:40.840476Z","shell.execute_reply":"2023-12-02T22:05:40.840492Z"},"trusted":true,"id":"cSGMb3SyNEzS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### A plot to visualize every neuron"],"metadata":{"id":"vawYxNApNEzS"}},{"cell_type":"code","source":["# generator = Visualize_Generator(in_ch=3, out_ch=3)\n","\n","# # Set the model to training mode (important for dropout layers)\n","# generator.train()\n","\n","# # Load the image from the specified path\n","# image_path = \"../input/gan-getting-started/photo_jpg/00068bc07f.jpg\"\n","# image = Image.open(image_path).convert(\"RGB\")\n","\n","# # Define image transformations to match the generator's input requirements\n","# transform = transforms.Compose([\n","#     transforms.Resize((256, 256)),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","# ])\n","\n","# # Apply transformations to the image\n","# input_image = transform(image).unsqueeze(0).requires_grad_()  # Add batch dimension and require gradient\n","\n","# # Forward pass\n","# output_image = generator(input_image)\n","\n","# # Choose a specific layer for which you want to retain the gradients\n","# target_layer_index = 8  # Choose the index of the desired layer\n","\n","# # Get the number of neurons in the chosen layer\n","# num_neurons = list(generator.gen.children())[target_layer_index].out_channels\n","\n","# # Create subplots for each neuron\n","# fig, axes = plt.subplots(num_neurons + 1, 1, figsize=(200, 200))\n","\n","# # Display the original image\n","# axes[0].imshow(np.transpose(input_image.detach().numpy()[0], (1, 2, 0)))\n","# axes[0].set_title('Original Image')\n","# axes[0].axis('off')\n","\n","# # Backward pass and display gradients for each neuron\n","# for neuron_index in range(num_neurons):\n","#     # Backward pass for the specific neuron\n","#     output_image[:, neuron_index, :, :].sum().backward(retain_graph=True)\n","\n","#     # Get the gradients for the input image\n","#     input_image_gradients = input_image.grad.data.numpy()[0]\n","\n","#     # Normalize the gradients for visualization\n","#     input_image_gradients = (input_image_gradients - input_image_gradients.min()) / (\n","#         input_image_gradients.max() - input_image_gradients.min()\n","#     )\n","\n","#     # Display the gradients for the current neuron\n","#     axes[neuron_index + 1].imshow(np.transpose(input_image_gradients, (1, 2, 0)), cmap='gray')\n","#     axes[neuron_index + 1].set_title(f'Neuron {neuron_index}')\n","#     axes[neuron_index + 1].axis('off')\n","\n","# plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:05:40.841743Z","iopub.status.idle":"2023-12-02T22:05:40.842121Z","shell.execute_reply.started":"2023-12-02T22:05:40.841957Z","shell.execute_reply":"2023-12-02T22:05:40.841973Z"},"trusted":true,"id":"cDAPjpt1NEzS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Visualizing the output image after passing through the entire generator"],"metadata":{"id":"U7R1sKRKNEzT"}},{"cell_type":"code","source":["# generator = Visualize_Generator(in_ch=3, out_ch=3)\n","\n","# # Set the model to training mode (important for dropout layers)\n","# generator.train()\n","\n","# # Load the image from the specified path\n","# image_path = \"../input/gan-getting-started/photo_jpg/00068bc07f.jpg\"\n","# image = Image.open(image_path).convert(\"RGB\")\n","\n","# # Define image transformations to match the generator's input requirements\n","# transform = transforms.Compose([\n","#     transforms.Resize((256, 256)),\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","# ])\n","\n","# # Apply transformations to the image\n","# input_image = transform(image).unsqueeze(0).requires_grad_()  # Add batch dimension and require gradient\n","\n","# # Forward pass\n","# output_image = generator(input_image)\n","\n","\n","# fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","# axes[0].imshow(np.transpose(input_image.detach().numpy()[0], (1, 2, 0)))\n","# axes[0].set_title('Original Image')\n","# axes[0].axis('off')\n","# axes[1].imshow(np.transpose(output_image.detach().numpy()[0],(1, 2, 0)))\n","# axes[1].set_title('Output Image')\n","# axes[1].axis('off')\n","# plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:05:40.843444Z","iopub.status.idle":"2023-12-02T22:05:40.843771Z","shell.execute_reply.started":"2023-12-02T22:05:40.843601Z","shell.execute_reply":"2023-12-02T22:05:40.843616Z"},"trusted":true,"id":"5awKWps4NEzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# plot realistic photo and created monet-style image\n","_, ax = plt.subplots(3, 2, figsize=(12, 12))\n","for i in range(3):\n","    photo_img, _ = next(iter(img_dl))\n","    pred_monet = gan.gen_ptm(photo_img.to(device)).cpu().detach()\n","    photo_img = reverse_normalize(photo_img)\n","    pred_monet = reverse_normalize(pred_monet)\n","\n","    ax[i, 0].imshow(photo_img[0].permute(1, 2, 0))\n","    ax[i, 1].imshow(pred_monet[0].permute(1, 2, 0))\n","    ax[i, 0].set_title(\"Input Photo\")\n","    ax[i, 1].set_title(\"Monet-esque Photo\")\n","    ax[i, 0].axis(\"off\")\n","    ax[i, 1].axis(\"off\")\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T04:24:59.254903Z","iopub.execute_input":"2023-12-05T04:24:59.255864Z","iopub.status.idle":"2023-12-05T04:25:00.194694Z","shell.execute_reply.started":"2023-12-05T04:24:59.255824Z","shell.execute_reply":"2023-12-05T04:25:00.193620Z"},"trusted":true,"id":"3mK-ZXrKNEzT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can clearly see the difference and the expected monet-esque style. We talk about this in more detail in the Results and Analysis section."],"metadata":{"id":"3yGXAr99NEzT"}},{"cell_type":"code","source":["from glob import glob\n","_, ax = plt.subplots(7, 2, figsize=(6, 30))\n","\n","for i,image_path in enumerate(glob(\"../input/sample-images/*\")):\n","    #image_path = \"../input/sample-images/IMG-20231202-WA0005.jpg\"\n","    image = Image.open(image_path).convert(\"RGB\")\n","\n","    # Define image transformations to match the generator's input requirements\n","    transform = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","    input_image = transform(image).unsqueeze(0)\n","\n","\n","    #photo_img, _ = next(iter(img_dl))\n","    pred_monet = gan.gen_ptm(input_image.to(device)).cpu().detach()\n","    photo_img = reverse_normalize(input_image)\n","    pred_monet = reverse_normalize(pred_monet)\n","\n","    ax[i,0].imshow(photo_img[0].permute(1, 2, 0))\n","    ax[i,1].imshow(pred_monet[0].permute(1, 2, 0))\n","    ax[i,0].set_title(\"Input Photo\")\n","    ax[i,1].set_title(\"Monet-esque Photo\")\n","    ax[i,0].axis(\"off\")\n","    ax[i,1].axis(\"off\")\n","plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-05T04:19:26.410507Z","iopub.execute_input":"2023-12-05T04:19:26.410906Z","iopub.status.idle":"2023-12-05T04:19:28.673925Z","shell.execute_reply.started":"2023-12-05T04:19:26.410876Z","shell.execute_reply":"2023-12-05T04:19:28.672399Z"},"trusted":true,"id":"mQ_fTpsLNEzU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.5 Hyperparameter Tuning\n","\n","For this project, I decided to manually tune some hyperparameters. An automated tuning process would result in training times that were too long considering the running time of the whole notebook is already quite long. Since the available usage time for the GPU on Kaggle is limited, I had to make some compromise by tuning only the learning rate and the beta value of the optimizer manually. I tried out different combination during the project, but only left the ones yielding the best results in the code see before.\n","\n","The values are the following:\n","\n","* Starting learning rate: 2e-4\n","* Betas of optimizer: 0.5 and 0.999\n","\n","Considering the limited resources in this project the results seem to be quite good. Of course, in an industrial scenario the tuning process has to be more sophisticated."],"metadata":{"id":"M4JW5AoXNEzU"}},{"cell_type":"markdown","source":["## 5.6 Alternative loss functions\n","\n","During training I also experimented with an alternative optimizer, namely RMSprop instead of the Adam optimizer. RMSprop (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm. Is uses a moving average of the squared gradients to adjust the learning rate, while Adam also includes a term to correct for bias (momentum). There are two main advantages using RMSprop. First, is can be more stable than Adam in some cases, as it doesn't include the momentum term, which might result in smaller updates and reduce the likelihood of divergence during training. And second, RMSprop can lead to faster convergence in certain situations, especially when dealing with non-stationary or noisy gradients. However, the final results did not differ much from the Adam optimizer, which is why I kept the Adam version in the code."],"metadata":{"id":"tRie8svJNEzU"}},{"cell_type":"markdown","source":["# 6. Results and Analysis\n","\n","Now that we have trained our model we can use it to create the actual monet-like images from the provided photos in order to make a submission to the competition."],"metadata":{"id":"DlMthMIANEzU"}},{"cell_type":"code","source":["# class to store and access the photos (similar to the paintings done before)\n","class PhotoDataset(Dataset):\n","    def __init__(self, photo_dir, size=(256, 256), normalize=True):\n","        super().__init__()\n","        self.photo_dir = photo_dir\n","        self.photo_idx = dict()\n","\n","        # normalize images if needed\n","        if normalize:\n","            self.transform = transforms.Compose([\n","                transforms.Resize(size),\n","                transforms.ToTensor(),\n","                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","            ])\n","        else:\n","            self.transform = transforms.Compose([\n","                transforms.Resize(size),\n","                transforms.ToTensor()\n","            ])\n","        for i, fl in enumerate(os.listdir(self.photo_dir)):\n","            self.photo_idx[i] = fl\n","\n","    # retrieve an image\n","    def __getitem__(self, idx):\n","        photo_path = os.path.join(self.photo_dir, self.photo_idx[idx])\n","        photo_img = Image.open(photo_path)\n","        photo_img = self.transform(photo_img)\n","        return photo_img\n","\n","    def __len__(self):\n","        return len(self.photo_idx.keys())"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:07:15.658657Z","iopub.execute_input":"2023-12-02T22:07:15.659368Z","iopub.status.idle":"2023-12-02T22:07:15.668298Z","shell.execute_reply.started":"2023-12-02T22:07:15.659334Z","shell.execute_reply":"2023-12-02T22:07:15.667329Z"},"trusted":true,"id":"mk-Kgxk4NEzU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make a dataloader and the required directory for storing the images to be created\n","ph_ds = PhotoDataset(path_photo)\n","ph_dl = DataLoader(ph_ds, batch_size=1, pin_memory=True)\n","!mkdir ../images_v2\n","trans = transforms.ToPILImage()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:07:44.736717Z","iopub.execute_input":"2023-12-02T22:07:44.737666Z","iopub.status.idle":"2023-12-02T22:07:45.723114Z","shell.execute_reply.started":"2023-12-02T22:07:44.737628Z","shell.execute_reply":"2023-12-02T22:07:45.721995Z"},"trusted":true,"id":"tNbvLcQ8NEzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# use model to create a monet style image\n","t = tqdm(ph_dl, leave=False, total=ph_dl.__len__())\n","for i, photo in enumerate(t):\n","    with torch.no_grad():\n","        pred_monet = gan.gen_ptm(photo.to(device)).cpu().detach()\n","\n","    # revert the normalization process to obtain the original image style\n","    pred_monet = reverse_normalize(pred_monet)\n","    img = trans(pred_monet[0]).convert(\"RGB\")\n","\n","    # store the image\n","    img.save(\"../images_v2/\" + str(i+1) + \".jpg\")"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:07:51.978370Z","iopub.execute_input":"2023-12-02T22:07:51.978732Z","iopub.status.idle":"2023-12-02T22:09:38.286062Z","shell.execute_reply.started":"2023-12-02T22:07:51.978704Z","shell.execute_reply":"2023-12-02T22:09:38.285144Z"},"trusted":true,"id":"lXMaY2NYNEzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a zip file from all created images\n","shutil.make_archive(\"/kaggle/working/images_v2\", 'zip', \"/kaggle/images_v2\")"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:10:50.266150Z","iopub.execute_input":"2023-12-02T22:10:50.266516Z","iopub.status.idle":"2023-12-02T22:10:54.826420Z","shell.execute_reply.started":"2023-12-02T22:10:50.266487Z","shell.execute_reply":"2023-12-02T22:10:54.825494Z"},"trusted":true,"id":"gwt_hVSbNEzV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The generated zip file will be used by Kaggle to determine our place on the competition leaderboard. We can have a look at the images created for the submission. We can clearly recognize the typical monet style."],"metadata":{"id":"VUwd5jSoNEzV"}},{"cell_type":"code","source":["def display_sample_images(path, num_samples=16):\n","    sample_images = os.listdir(path)[:num_samples]\n","\n","    w = int(num_samples ** .5)\n","    h = math.ceil(num_samples / w)\n","\n","    for ind, image_name in enumerate(sample_images):\n","        img = cv2.imread(os.path.join(path, image_name))\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        plt.subplot(h, w, ind + 1)\n","        plt.imshow(img)\n","        plt.axis(\"off\")\n","\n","    plt.show()"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:10:59.569672Z","iopub.execute_input":"2023-12-02T22:10:59.570048Z","iopub.status.idle":"2023-12-02T22:10:59.576469Z","shell.execute_reply.started":"2023-12-02T22:10:59.570018Z","shell.execute_reply":"2023-12-02T22:10:59.575529Z"},"trusted":true,"id":"J5u8uCQQNEzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display_sample_images(\"../images_v2/\")"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:11:04.699342Z","iopub.execute_input":"2023-12-02T22:11:04.699714Z","iopub.status.idle":"2023-12-02T22:11:05.370392Z","shell.execute_reply.started":"2023-12-02T22:11:04.699684Z","shell.execute_reply":"2023-12-02T22:11:05.369532Z"},"trusted":true,"id":"qlpdTpnONEzW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# display best hyperparameters of model\n","learning_rate = [2e-4]\n","beta_1 = [0.5]\n","beta_2 = [0.999]\n","df_results = pd.DataFrame({'Learning Rate': learning_rate, 'Beta 1': beta_1, 'Beta 2': beta_2})\n","print(df_results)"],"metadata":{"execution":{"iopub.status.busy":"2023-12-02T22:11:09.057429Z","iopub.execute_input":"2023-12-02T22:11:09.058162Z","iopub.status.idle":"2023-12-02T22:11:09.074626Z","shell.execute_reply.started":"2023-12-02T22:11:09.058128Z","shell.execute_reply":"2023-12-02T22:11:09.073764Z"},"trusted":true,"id":"2EtfX5hMNEzW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As we can see from the resulting images during training and testing (= create submission images) we can confidently say that our model performed quite well with the shown set of hyperparameters above. If we had we more computing power we could automate this process of hyperparameter tuning even further to optimize the results. But for the context of this project the results are good enough thanks to the underlying CycleGAN model.\n","\n","Since CycleGAN is effective for a wide range of image-to-image translation tasks, such as turning photos into paintings or converting day scenes to night scenes it also worked out well for our use case. Additionally, the adversarial training process helps generate high-quality, realistic images. Of course, there are other models we could explore. For example U-Net which is excellent for tasks that involve dense pixel-to-pixel correspondence, such as image segmentation. It even has a simpler architecture compared to GANs."],"metadata":{"id":"gSlbN1ThNEzW"}},{"cell_type":"markdown","source":["# 7. Conclusion\n","\n","## 7.1 Result Summary\n","\n","As we have already seen in the previous section, the CycleGAN network works really well with our images and the shown set of hyperparameters."],"metadata":{"id":"tGIroxFnNEzW"}},{"cell_type":"markdown","source":["## 7.2 Learnings and Takeaways\n","\n","Working with neural networks involves a distinct workflow compared to simpler supervised learning methods like regression analysis. Training models in this context can be time-consuming, sometimes taking hours. Therefore, maintaining a clean and precise work process is crucial to avoid excessive training iterations. An often underestimated but highly valuable practice in this regard is the strategic use of print statements within your code. These statements enable effective debugging, inspection of intermediate results, and quicker error identification. Another challenge I encountered during my work was the constrained memory and time resources of the GPU provided by Kaggle."],"metadata":{"id":"BUSMctW0NEzW"}},{"cell_type":"markdown","source":["## 7.3 What didn't work\n","\n","Overall, everything worked quite well. However, it would have been great to work with more epochs in order to get more precise results or perform an automated and more sohpisticated hyperparameter tuning. The limited GPU time really makes things a bit hard since processing image data requires so much disk space and computation time. It took quite a few iterations to find good values for the hyperparameters."],"metadata":{"id":"VOuONy-tNEzW"}},{"cell_type":"markdown","source":["## 7.4 Possible improvements\n","\n","If we had enough computing power, we could expand our hyperparameter tuning by including more parameters and a wider value range. We could also experiment with totally different model and compare their results."],"metadata":{"id":"P96tlz8hNEzX"}}]}